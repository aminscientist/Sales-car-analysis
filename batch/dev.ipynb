{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-01-29T19:09:36.162799900Z",
     "start_time": "2024-01-29T19:09:35.782329600Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "['cars', 'rmstate']"
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from hdfs import InsecureClient\n",
    "\n",
    "client = InsecureClient(\"http://localhost:9870\", user=\"amine\")\n",
    "\n",
    "client.list(\"/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "client.write(\"/cars/text.txt\", \"Hello world\", overwrite=True)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-29T18:42:55.112930800Z",
     "start_time": "2024-01-29T18:42:49.472434300Z"
    }
   },
   "id": "8da8fc6db42fc14c"
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "data": {
      "text/plain": "   Column1 Column2\n0        1       A\n1        2       B\n2        3       C",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Column1</th>\n      <th>Column2</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>A</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2</td>\n      <td>B</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>3</td>\n      <td>C</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from hdfs import InsecureClient\n",
    "import pyarrow as pa\n",
    "\n",
    "# Create a sample DataFrame (replace this with your actual DataFrame)\n",
    "data = {'Column1': [1, 2, 3], 'Column2': ['A', 'B', 'C']}\n",
    "df = pd.DataFrame(data)\n",
    "df"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-29T19:16:24.576322600Z",
     "start_time": "2024-01-29T19:16:22.518959200Z"
    }
   },
   "id": "398062d32cc6db7f"
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [],
   "source": [
    "with client.write(\"/cars/data.csv\", encoding='utf-8') as writer:\n",
    "    df.to_csv(writer)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-29T19:17:23.237401500Z",
     "start_time": "2024-01-29T19:17:20.306918100Z"
    }
   },
   "id": "4e01f69d6a0e69cf"
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'pyarrow' has no attribute 'parquet'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mAttributeError\u001B[0m                            Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[18], line 23\u001B[0m\n\u001B[0;32m     21\u001B[0m \u001B[38;5;66;03m# Write PyArrow Table to HDFS in Parquet format\u001B[39;00m\n\u001B[0;32m     22\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m client\u001B[38;5;241m.\u001B[39mwrite(hdfs_path, overwrite\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m) \u001B[38;5;28;01mas\u001B[39;00m writer:\n\u001B[1;32m---> 23\u001B[0m     \u001B[43mpa\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mparquet\u001B[49m\u001B[38;5;241m.\u001B[39mwrite_table(table, writer)\n",
      "File \u001B[1;32m~\\Desktop\\Sales-car-analysis\\lib\\site-packages\\pyarrow\\__init__.py:317\u001B[0m, in \u001B[0;36m__getattr__\u001B[1;34m(name)\u001B[0m\n\u001B[0;32m    313\u001B[0m     _warnings\u001B[38;5;241m.\u001B[39mwarn(_msg\u001B[38;5;241m.\u001B[39mformat(name, new_name),\n\u001B[0;32m    314\u001B[0m                    \u001B[38;5;167;01mFutureWarning\u001B[39;00m, stacklevel\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m2\u001B[39m)\n\u001B[0;32m    315\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m obj\n\u001B[1;32m--> 317\u001B[0m \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mAttributeError\u001B[39;00m(\n\u001B[0;32m    318\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmodule \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mpyarrow\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m has no attribute \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;132;01m{0}\u001B[39;00m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;241m.\u001B[39mformat(name)\n\u001B[0;32m    319\u001B[0m )\n",
      "\u001B[1;31mAttributeError\u001B[0m: module 'pyarrow' has no attribute 'parquet'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from hdfs import InsecureClient\n",
    "import pyarrow as pa\n",
    "\n",
    "# Create a sample DataFrame (replace this with your actual DataFrame)\n",
    "data = {'Column1': [1, 2, 3, 4], 'Column2': ['A', 'B', 'C', 'D']}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Specify HDFS parameters\n",
    "hdfs_host = \"http://localhost\"\n",
    "hdfs_port = 9870\n",
    "hdfs_user = \"amine\"\n",
    "hdfs_path = \"/cars/data1.parquet\"\n",
    "\n",
    "# Connect to HDFS\n",
    "client = InsecureClient(f\"{hdfs_host}:{hdfs_port}\", user=hdfs_user)\n",
    "\n",
    "# Convert DataFrame to PyArrow Table\n",
    "table = pa.Table.from_pandas(df)\n",
    "\n",
    "# Write PyArrow Table to HDFS in Parquet format\n",
    "with client.write(hdfs_path, overwrite=True) as writer:\n",
    "    pa.parquet.write_table(table, writer)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-29T19:19:39.552963400Z",
     "start_time": "2024-01-29T19:19:35.544057400Z"
    }
   },
   "id": "b7d6673b910e7410"
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [
    {
     "ename": "ArrowInvalid",
     "evalue": "Could not open Parquet input source '<Buffer>': Parquet file size is 0 bytes",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mArrowInvalid\u001B[0m                              Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[19], line 1\u001B[0m\n\u001B[1;32m----> 1\u001B[0m df \u001B[38;5;241m=\u001B[39m \u001B[43mpd\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mread_parquet\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mdata1.parquet\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[0;32m      2\u001B[0m df\n",
      "File \u001B[1;32m~\\Desktop\\Sales-car-analysis\\lib\\site-packages\\pandas\\io\\parquet.py:670\u001B[0m, in \u001B[0;36mread_parquet\u001B[1;34m(path, engine, columns, storage_options, use_nullable_dtypes, dtype_backend, filesystem, filters, **kwargs)\u001B[0m\n\u001B[0;32m    667\u001B[0m     use_nullable_dtypes \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mFalse\u001B[39;00m\n\u001B[0;32m    668\u001B[0m check_dtype_backend(dtype_backend)\n\u001B[1;32m--> 670\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m impl\u001B[38;5;241m.\u001B[39mread(\n\u001B[0;32m    671\u001B[0m     path,\n\u001B[0;32m    672\u001B[0m     columns\u001B[38;5;241m=\u001B[39mcolumns,\n\u001B[0;32m    673\u001B[0m     filters\u001B[38;5;241m=\u001B[39mfilters,\n\u001B[0;32m    674\u001B[0m     storage_options\u001B[38;5;241m=\u001B[39mstorage_options,\n\u001B[0;32m    675\u001B[0m     use_nullable_dtypes\u001B[38;5;241m=\u001B[39muse_nullable_dtypes,\n\u001B[0;32m    676\u001B[0m     dtype_backend\u001B[38;5;241m=\u001B[39mdtype_backend,\n\u001B[0;32m    677\u001B[0m     filesystem\u001B[38;5;241m=\u001B[39mfilesystem,\n\u001B[0;32m    678\u001B[0m     \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs,\n\u001B[0;32m    679\u001B[0m )\n",
      "File \u001B[1;32m~\\Desktop\\Sales-car-analysis\\lib\\site-packages\\pandas\\io\\parquet.py:272\u001B[0m, in \u001B[0;36mPyArrowImpl.read\u001B[1;34m(self, path, columns, filters, use_nullable_dtypes, dtype_backend, storage_options, filesystem, **kwargs)\u001B[0m\n\u001B[0;32m    265\u001B[0m path_or_handle, handles, filesystem \u001B[38;5;241m=\u001B[39m _get_path_or_handle(\n\u001B[0;32m    266\u001B[0m     path,\n\u001B[0;32m    267\u001B[0m     filesystem,\n\u001B[0;32m    268\u001B[0m     storage_options\u001B[38;5;241m=\u001B[39mstorage_options,\n\u001B[0;32m    269\u001B[0m     mode\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mrb\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[0;32m    270\u001B[0m )\n\u001B[0;32m    271\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m--> 272\u001B[0m     pa_table \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mapi\u001B[38;5;241m.\u001B[39mparquet\u001B[38;5;241m.\u001B[39mread_table(\n\u001B[0;32m    273\u001B[0m         path_or_handle,\n\u001B[0;32m    274\u001B[0m         columns\u001B[38;5;241m=\u001B[39mcolumns,\n\u001B[0;32m    275\u001B[0m         filesystem\u001B[38;5;241m=\u001B[39mfilesystem,\n\u001B[0;32m    276\u001B[0m         filters\u001B[38;5;241m=\u001B[39mfilters,\n\u001B[0;32m    277\u001B[0m         \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs,\n\u001B[0;32m    278\u001B[0m     )\n\u001B[0;32m    279\u001B[0m     result \u001B[38;5;241m=\u001B[39m pa_table\u001B[38;5;241m.\u001B[39mto_pandas(\u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mto_pandas_kwargs)\n\u001B[0;32m    281\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m manager \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124marray\u001B[39m\u001B[38;5;124m\"\u001B[39m:\n",
      "File \u001B[1;32m~\\Desktop\\Sales-car-analysis\\lib\\site-packages\\pyarrow\\parquet\\core.py:1776\u001B[0m, in \u001B[0;36mread_table\u001B[1;34m(source, columns, use_threads, schema, use_pandas_metadata, read_dictionary, memory_map, buffer_size, partitioning, filesystem, filters, use_legacy_dataset, ignore_prefixes, pre_buffer, coerce_int96_timestamp_unit, decryption_properties, thrift_string_size_limit, thrift_container_size_limit, page_checksum_verification)\u001B[0m\n\u001B[0;32m   1770\u001B[0m     warnings\u001B[38;5;241m.\u001B[39mwarn(\n\u001B[0;32m   1771\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mPassing \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124muse_legacy_dataset\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m is deprecated as of pyarrow 15.0.0 \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m   1772\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mand will be removed in a future version.\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[0;32m   1773\u001B[0m         \u001B[38;5;167;01mFutureWarning\u001B[39;00m, stacklevel\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m2\u001B[39m)\n\u001B[0;32m   1775\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m-> 1776\u001B[0m     dataset \u001B[38;5;241m=\u001B[39m \u001B[43mParquetDataset\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m   1777\u001B[0m \u001B[43m        \u001B[49m\u001B[43msource\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1778\u001B[0m \u001B[43m        \u001B[49m\u001B[43mschema\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mschema\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1779\u001B[0m \u001B[43m        \u001B[49m\u001B[43mfilesystem\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mfilesystem\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1780\u001B[0m \u001B[43m        \u001B[49m\u001B[43mpartitioning\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mpartitioning\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1781\u001B[0m \u001B[43m        \u001B[49m\u001B[43mmemory_map\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmemory_map\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1782\u001B[0m \u001B[43m        \u001B[49m\u001B[43mread_dictionary\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mread_dictionary\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1783\u001B[0m \u001B[43m        \u001B[49m\u001B[43mbuffer_size\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mbuffer_size\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1784\u001B[0m \u001B[43m        \u001B[49m\u001B[43mfilters\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mfilters\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1785\u001B[0m \u001B[43m        \u001B[49m\u001B[43mignore_prefixes\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mignore_prefixes\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1786\u001B[0m \u001B[43m        \u001B[49m\u001B[43mpre_buffer\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mpre_buffer\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1787\u001B[0m \u001B[43m        \u001B[49m\u001B[43mcoerce_int96_timestamp_unit\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcoerce_int96_timestamp_unit\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1788\u001B[0m \u001B[43m        \u001B[49m\u001B[43mthrift_string_size_limit\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mthrift_string_size_limit\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1789\u001B[0m \u001B[43m        \u001B[49m\u001B[43mthrift_container_size_limit\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mthrift_container_size_limit\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1790\u001B[0m \u001B[43m        \u001B[49m\u001B[43mpage_checksum_verification\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mpage_checksum_verification\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1791\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1792\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mImportError\u001B[39;00m:\n\u001B[0;32m   1793\u001B[0m     \u001B[38;5;66;03m# fall back on ParquetFile for simple cases when pyarrow.dataset\u001B[39;00m\n\u001B[0;32m   1794\u001B[0m     \u001B[38;5;66;03m# module is not available\u001B[39;00m\n\u001B[0;32m   1795\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m filters \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n",
      "File \u001B[1;32m~\\Desktop\\Sales-car-analysis\\lib\\site-packages\\pyarrow\\parquet\\core.py:1343\u001B[0m, in \u001B[0;36mParquetDataset.__init__\u001B[1;34m(self, path_or_paths, filesystem, schema, filters, read_dictionary, memory_map, buffer_size, partitioning, ignore_prefixes, pre_buffer, coerce_int96_timestamp_unit, decryption_properties, thrift_string_size_limit, thrift_container_size_limit, page_checksum_verification, use_legacy_dataset)\u001B[0m\n\u001B[0;32m   1339\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m single_file \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m   1340\u001B[0m     fragment \u001B[38;5;241m=\u001B[39m parquet_format\u001B[38;5;241m.\u001B[39mmake_fragment(single_file, filesystem)\n\u001B[0;32m   1342\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_dataset \u001B[38;5;241m=\u001B[39m ds\u001B[38;5;241m.\u001B[39mFileSystemDataset(\n\u001B[1;32m-> 1343\u001B[0m         [fragment], schema\u001B[38;5;241m=\u001B[39mschema \u001B[38;5;129;01mor\u001B[39;00m \u001B[43mfragment\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mphysical_schema\u001B[49m,\n\u001B[0;32m   1344\u001B[0m         \u001B[38;5;28mformat\u001B[39m\u001B[38;5;241m=\u001B[39mparquet_format,\n\u001B[0;32m   1345\u001B[0m         filesystem\u001B[38;5;241m=\u001B[39mfragment\u001B[38;5;241m.\u001B[39mfilesystem\n\u001B[0;32m   1346\u001B[0m     )\n\u001B[0;32m   1347\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m\n\u001B[0;32m   1349\u001B[0m \u001B[38;5;66;03m# check partitioning to enable dictionary encoding\u001B[39;00m\n",
      "File \u001B[1;32m~\\Desktop\\Sales-car-analysis\\lib\\site-packages\\pyarrow\\_dataset.pyx:1367\u001B[0m, in \u001B[0;36mpyarrow._dataset.Fragment.physical_schema.__get__\u001B[1;34m()\u001B[0m\n",
      "File \u001B[1;32m~\\Desktop\\Sales-car-analysis\\lib\\site-packages\\pyarrow\\error.pxi:154\u001B[0m, in \u001B[0;36mpyarrow.lib.pyarrow_internal_check_status\u001B[1;34m()\u001B[0m\n",
      "File \u001B[1;32m~\\Desktop\\Sales-car-analysis\\lib\\site-packages\\pyarrow\\error.pxi:91\u001B[0m, in \u001B[0;36mpyarrow.lib.check_status\u001B[1;34m()\u001B[0m\n",
      "\u001B[1;31mArrowInvalid\u001B[0m: Could not open Parquet input source '<Buffer>': Parquet file size is 0 bytes"
     ]
    }
   ],
   "source": [
    "df = pd.read_parquet('data1.parquet')\n",
    "df"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-29T19:22:55.374811300Z",
     "start_time": "2024-01-29T19:22:52.219298900Z"
    }
   },
   "id": "a3d94a308a913318"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "b013b6db946172d"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
